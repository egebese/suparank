---
import Layout from '../../layouts/Layout.astro';
import Header from '../../components/Header.astro';
import Footer from '../../components/Footer.astro';

const tool = {
  id: 'robots-txt-generator',
  name: 'Robots.txt Generator',
  command: '/suparank-robots',
  description: 'Free robots.txt generator with AI crawler rules. Create optimized robots.txt files with sitemap references, GPTBot rules, and framework-specific settings.',
  longDescription: 'Generate a properly configured robots.txt file directly in your AI coding assistant. Includes AI crawler directives for GPTBot (OpenAI), Google-Extended, CCBot, and anthropic-ai. Automatic framework detection for Next.js, Astro, React, and WordPress. Add sitemap references, block admin paths, and optimize crawl budget.',
  category: 'Technical',
  icon: 'ph-duotone ph-file-code'
};

const features = [
  { name: 'Framework Detection', description: 'Auto-detects Next.js, Astro, WordPress, and other frameworks' },
  { name: 'AI Crawler Rules', description: 'Include rules for GPTBot, ClaudeBot, PerplexityBot, Google-Extended' },
  { name: 'Path Analysis', description: 'Identifies admin, API, and build paths to block automatically' },
  { name: 'Sitemap Reference', description: 'Adds proper sitemap directive for search engines' },
  { name: 'llms.txt Reference', description: 'Includes llms.txt comment for AI content discovery' },
  { name: 'Best Practices', description: 'Follows Yoast, Google, and industry recommendations' }
];

const faqs = [
  {
    question: 'What is a robots.txt file?',
    answer: 'A robots.txt file is a text file placed in your website\'s root directory that tells search engine crawlers which pages or sections of your site to crawl or not crawl. It\'s part of the Robots Exclusion Protocol (REP) and is the first file crawlers check when visiting your site. Modern robots.txt files also include sitemap references and AI crawler directives.'
  },
  {
    question: 'How do I block AI crawlers like GPTBot?',
    answer: 'To block GPTBot (OpenAI\'s crawler), add "User-agent: GPTBot" followed by "Disallow: /" in your robots.txt. You can also block Google-Extended (Google\'s AI training), CCBot (Common Crawl), and anthropic-ai (Claude). Each AI company uses different user agents, so you need separate rules for each one you want to block.'
  },
  {
    question: 'Should I add sitemap to robots.txt?',
    answer: 'Yes, adding your sitemap URL to robots.txt helps search engines discover all your pages faster. Use the format "Sitemap: https://yourdomain.com/sitemap.xml". You can list multiple sitemaps if needed. This is especially important for large sites or when you have dynamic content that changes frequently.'
  },
  {
    question: 'What paths should I block in robots.txt?',
    answer: 'Commonly blocked paths include admin areas (/admin/, /wp-admin/), API endpoints (/api/), build directories (/_next/, /_astro/), temporary files, search results (?s=), and user-specific pages. Blocking these paths helps optimize crawl budget by focusing crawlers on your important content pages. Never block CSS, JavaScript, or images as this can hurt SEO.'
  }
];

const softwareSchema = {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "Suparank Robots.txt Generator",
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Cross-platform",
  "description": tool.longDescription,
  "url": `https://suparank.io/tools/${tool.id}`,
  "author": { "@type": "Organization", "name": "Suparank" },
  "offers": { "@type": "Offer", "price": "0", "priceCurrency": "USD" }
};

const faqSchema = {
  "@context": "https://schema.org",
  "@type": "FAQPage",
  "mainEntity": faqs.map(faq => ({
    "@type": "Question",
    "name": faq.question,
    "acceptedAnswer": { "@type": "Answer", "text": faq.answer }
  }))
};

const exampleOutput = `# Suparank - https://suparank.io
# Generated by Suparank robots.txt Generator

User-agent: *
Allow: /

# Block non-content paths
Disallow: /api/
Disallow: /_astro/

# AI Crawlers - Allow for AI search visibility
User-agent: GPTBot
Allow: /

User-agent: ClaudeBot
Allow: /

User-agent: Google-Extended
Allow: /

User-agent: PerplexityBot
Allow: /

# Sitemap
Sitemap: https://suparank.io/sitemap.xml

# LLMs.txt for AI content discovery
# https://suparank.io/llms.txt
# Full version: https://suparank.io/llms-full.txt`;
---

<Layout
  title="Free Robots.txt Generator - AI Crawler Rules & Sitemap | Suparank"
  description="Free robots.txt generator with GPTBot and AI crawler rules. Create optimized robots.txt with sitemap references, framework detection for Next.js, Astro, WordPress."
>
  <Fragment slot="head">
    <script type="application/ld+json" set:html={JSON.stringify(softwareSchema)} />
    <script type="application/ld+json" set:html={JSON.stringify(faqSchema)} />
  </Fragment>
  <Header />
  <main>
    <!-- Hero -->
    <section class="pt-24 md:pt-32 pb-16 section-divider">
      <div class="container mx-auto px-6">
        <div class="max-w-4xl mx-auto">
          <a href="/tools" class="inline-flex items-center gap-2 text-sm text-muted-foreground hover:text-foreground mb-6">
            <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7" />
            </svg>
            All Tools
          </a>

          <div class="flex items-center gap-4 mb-6">
            <i class={`${tool.icon} text-5xl text-primary`}></i>
            <span class="text-xs font-medium px-2 py-1 bg-primary/10 text-primary rounded-full">{tool.category}</span>
          </div>

          <h1 class="text-4xl md:text-5xl font-bold tracking-tight mb-4">{tool.name}</h1>
          <p class="text-xl text-muted-foreground mb-8">{tool.longDescription}</p>

          <div class="bg-muted/50 border rounded p-4 mb-6">
            <p class="text-sm text-muted-foreground mb-2">Install and use:</p>
            <div class="space-y-2">
              <div class="flex items-center gap-2 bg-background rounded border px-4 py-2 font-mono text-sm">
                <code class="flex-1">curl -fsSL https://suparank.io/install | bash</code>
              </div>
              <div class="flex items-center gap-2 bg-background rounded border px-4 py-2 font-mono text-sm">
                <code class="flex-1 text-primary">{tool.command}</code>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Features -->
    <section class="py-16 section-divider bg-muted/30">
      <div class="container mx-auto px-6">
        <div class="max-w-4xl mx-auto">
          <h2 class="text-2xl font-bold mb-8">Features</h2>
          <div class="grid md:grid-cols-2 gap-4">
            {features.map(feature => (
              <div class="p-4 bg-background border rounded">
                <h3 class="font-semibold mb-1">{feature.name}</h3>
                <p class="text-sm text-muted-foreground">{feature.description}</p>
              </div>
            ))}
          </div>
        </div>
      </div>
    </section>

    <!-- Example Output -->
    <section class="py-16 section-divider">
      <div class="container mx-auto px-6">
        <div class="max-w-4xl mx-auto">
          <h2 class="text-2xl font-bold mb-8">Example robots.txt Output</h2>
          <div class="bg-zinc-950 text-zinc-100 rounded p-6 overflow-x-auto">
            <pre class="text-sm font-mono whitespace-pre">{exampleOutput}</pre>
          </div>
        </div>
      </div>
    </section>

    <!-- FAQ -->
    <section class="py-16 section-divider bg-muted/30">
      <div class="container mx-auto px-6">
        <div class="max-w-3xl mx-auto">
          <h2 class="text-2xl font-bold mb-8">Frequently Asked Questions</h2>
          <div class="space-y-4">
            {faqs.map(faq => (
              <div class="border rounded p-6 bg-background">
                <h3 class="font-semibold mb-2">{faq.question}</h3>
                <p class="text-muted-foreground">{faq.answer}</p>
              </div>
            ))}
          </div>
        </div>
      </div>
    </section>

    <!-- Related Tools -->
    <section class="py-16 section-divider">
      <div class="container mx-auto px-6">
        <div class="max-w-4xl mx-auto">
          <h2 class="text-2xl font-bold mb-8">Related Tools</h2>
          <div class="grid md:grid-cols-3 gap-4">
            <a href="/tools/llms-txt-generator" class="p-4 border rounded hover:border-primary/50 transition-colors">
              <i class="ph-duotone ph-robot text-2xl text-primary"></i>
              <h3 class="font-semibold mt-2">LLMs.txt Generator</h3>
              <p class="text-sm text-muted-foreground">AI search optimization</p>
            </a>
            <a href="/tools/technical-seo-audit" class="p-4 border rounded hover:border-primary/50 transition-colors">
              <i class="ph-duotone ph-gear-six text-2xl text-primary"></i>
              <h3 class="font-semibold mt-2">Technical SEO Audit</h3>
              <p class="text-sm text-muted-foreground">Crawlability & indexing</p>
            </a>
            <a href="/tools/schema-markup-generator" class="p-4 border rounded hover:border-primary/50 transition-colors">
              <i class="ph-duotone ph-brackets-curly text-2xl text-primary"></i>
              <h3 class="font-semibold mt-2">Schema Markup Generator</h3>
              <p class="text-sm text-muted-foreground">JSON-LD structured data</p>
            </a>
          </div>
        </div>
      </div>
    </section>

    <!-- CTA -->
    <section class="py-16 section-divider bg-primary/5">
      <div class="container mx-auto px-6">
        <div class="max-w-3xl mx-auto text-center">
          <h2 class="text-2xl font-bold mb-4">Generate Your robots.txt</h2>
          <p class="text-muted-foreground mb-6">Install and create an optimized robots.txt in seconds.</p>
          <div class="bg-background border rounded p-4 max-w-xl mx-auto">
            <code class="font-mono">curl -fsSL https://suparank.io/install | bash</code>
          </div>
        </div>
      </div>
    </section>
  </main>
  <Footer />
</Layout>
